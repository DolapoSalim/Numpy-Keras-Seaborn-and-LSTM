# -*- coding: utf-8 -*-
"""Numpy, Seaborn, Keras and LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nuztgzFWZ5lO16hVrBbKY_4zAz_Rk1MA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

random_array = np.random.rand(6, 9)
print (random_array)

df = pd.DataFrame (random_array, columns=['Cat', 'Dog', 'Duck', 'goat', 'ram', 'turkey', 'hen','lizard','Monkey'], index = ['temperature', 'Salinity', 'Turbidity', 'Deep', 'Light', 'Dissolved_Oxygen'])
print(df)

df.plot()
plt.show()

df['Cat'].plot()
plt.show()

# Calculate statistical features
cat_data = df['Cat']
mean = cat_data.mean()
median = cat_data.median()
mode = cat_data.mode()[0]  # Mode can have multiple values, take the first one
std_dev = cat_data.std()
variance = cat_data.var()

# Calculate degrees of freedom (assuming it's related to sample size)
degrees_of_freedom = len(cat_data) - 1

# Plotting the 'Cat' column with statistics
plt.figure(figsize=(10, 6))  # Adjust figure size if needed
plt.plot(df.index, cat_data, label='Cat Data', marker='o')  # Added markers for better visualization

# Add statistical information to the plot
plt.text(0.05, 0.95, f"Mean: {mean:.2f}", transform=plt.gca().transAxes, fontsize=12)
plt.text(0.05, 0.90, f"Median: {median:.2f}", transform=plt.gca().transAxes, fontsize=12)
plt.text(0.05, 0.85, f"Mode: {mode:.2f}", transform=plt.gca().transAxes, fontsize=12)
plt.text(0.05, 0.80, f"Std Dev: {std_dev:.2f}", transform=plt.gca().transAxes, fontsize=12)
plt.text(0.05, 0.75, f"Variance: {variance:.2f}", transform=plt.gca().transAxes, fontsize=12)
plt.text(0.05, 0.70, f"Degrees of Freedom: {degrees_of_freedom}", transform=plt.gca().transAxes, fontsize=12)

sns.boxplot(y=cat_data)
plt.title("Box Plot of Cat Data")
plt.show()

!pip install statsmodels
import statsmodels.formula.api as smf  # Import the necessary module

# Generate random data
np.random.seed(42)  # Set seed for reproducibility

# Number of groups and observations per group
num_groups = 5
num_obs_per_group = 10

# Create group and observation IDs
group_ids = np.repeat(np.arange(1, num_groups + 1), num_obs_per_group)
obs_ids = np.tile(np.arange(1, num_obs_per_group + 1), num_groups)

# Generate random predictor and response variables
predictor = np.random.rand(num_groups * num_obs_per_group)
response = 2 * predictor + np.random.randn(num_groups * num_obs_per_group) + group_ids

# Create a Pandas DataFrame
data = pd.DataFrame({'group': group_ids, 'obs': obs_ids, 'predictor': predictor, 'response': response})
data

# Define and fit the mixed model
model = smf.mixedlm("response ~ predictor", data, groups=data["group"], re_formula="~predictor")
result = model.fit()

# Print the model summary
print(result.summary())

!pip install pygments

import numpy as np
import pandas as pd
import statsmodels.formula.api as smf
from pygments import highlight
from pygments.lexers import PythonLexer
from pygments.formatters import HtmlFormatter
from IPython.display import HTML

# Get the model summary as a string
summary_str = result.summary().as_text()

# Highlight the summary using pygments
highlighted_summary = highlight(summary_str, PythonLexer(), HtmlFormatter())

# Display the highlighted summary as HTML
display(HTML(highlighted_summary))

import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

def create_dataset(dataset, look_back=1):
  X, Y = [], []
  for i in range(len(dataset)-look_back-1):
    a = dataset[i:(i+look_back), 0]
    X.append(a)
    Y.append(dataset[i + look_back, 0])
  return np.array(X), np.array(Y)

# Generate sample data
data = np.arange(0, 200, 1)
data = data.reshape(-1, 1)

# Scale data between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data)

# Split into train and test sets
train_size = int(len(data) * 0.67)
test_size = len(data) - train_size
train, test = data[0:train_size,:], data[train_size:len(data),:]

# Reshape into X=t and Y=t+1
look_back = 1
X_train, y_train = create_dataset(train, look_back)
X_test, y_test = create_dataset(test, look_back)

# Reshape input to be [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

# Create and fit the LSTM network
model = keras.Sequential()
model.add(LSTM(units=4, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)

# Make predictions
trainPredict = model.predict(X_train)
testPredict = model.predict(X_test)

# Invert predictions back to original scale
trainPredict = scaler.inverse_transform(trainPredict)
y_train = scaler.inverse_transform([y_train])
testPredict = scaler.inverse_transform(testPredict)
y_test = scaler.inverse_transform([y_test])

# Calculate root mean squared error
trainScore = np.sqrt(mean_squared_error(y_train[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = np.sqrt(mean_squared_error(y_test[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

# Generate synthetic temperature data
np.random.seed(42)  # For reproducibility
num_days = 365
temperatures = np.linspace(15, 25, num_days) + np.random.normal(0, 2, num_days)
data = temperatures.reshape(-1, 1)

# Scale data between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data)

# Split into train and test sets
train_size = int(len(data) * 0.67)
test_size = len(data) - train_size
train, test = data[0:train_size,:], data[train_size:len(data),:]

# Create dataset with look_back
look_back = 7  # Use the past 7 days to predict the next day's temperature
X_train, y_train = create_dataset(train, look_back)
X_test, y_test = create_dataset(test, look_back)

# Reshape input to be [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# Create and fit the LSTM network
model = keras.Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
model.add(LSTM(units=50))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)

# Make predictions
trainPredict = model.predict(X_train)
testPredict = model.predict(X_test)

# Invert predictions back to original scale
trainPredict = scaler.inverse_transform(trainPredict)
y_train = scaler.inverse_transform([y_train])
testPredict = scaler.inverse_transform(testPredict)
y_test = scaler.inverse_transform([y_test])

# Calculate root mean squared error
trainScore = np.sqrt(mean_squared_error(y_train[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = np.sqrt(mean_squared_error(y_test[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

new_data = np.array([18, 19, 20, 21, 22, 23, 24]) # Example temperatures for the past 7 days
new_data = new_data.reshape(-1, 1) # Reshape into a column vector
new_data = scaler.transform(new_data) # Scale using the trained scaler
new_data = new_data.reshape(1, look_back, 1) # Reshape for LSTM input

prediction = model.predict(new_data)

predicted_temperature = scaler.inverse_transform(prediction)
print("Predicted temperature for tomorrow:", predicted_temperature[0][0])